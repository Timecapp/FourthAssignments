{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Timecapp/FourthAssignments/blob/main/Copy_of_analyze_sentiment_subreddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5dyMb8xl4KJ"
      },
      "source": [
        "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
        "     width=\"200px\"\n",
        "     height=\"auto\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-srgOihl4KS"
      },
      "source": [
        "# <h1 align=\"center\" id=\"heading\">Sentiment Analysis of Reddit Data using Reddit API</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DgDgRE5l4KS"
      },
      "source": [
        "In this live coding session, we leverage the Python Reddit API Wrapper (`PRAW`) to retrieve data from subreddits on [Reddit](https://www.reddit.com), and perform sentiment analysis using [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) from [HuggingFace ( ü§ó the GitHub of Machine Learning )](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/), powered by [transformer](https://arxiv.org/pdf/1706.03762.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fty7pHnl4KS"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C21PYkQfl4KS"
      },
      "source": [
        "At the end of the session, you will "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snBfHOsll4KS"
      },
      "source": [
        "- know how to work with APIs\n",
        "- feel more comfortable navigating thru documentation, even inspecting the source code\n",
        "- understand what a `pipeline` object is in HuggingFace\n",
        "- perform sentiment analysis using `pipeline`\n",
        "- run a python script in command line and get the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "hNBiBX89l4KS"
      },
      "source": [
        "## How to Submit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "875XdA0Il4KT"
      },
      "source": [
        "- At the end of each task, commit* the work into the repository you created before the assignment\n",
        "- After completing all three tasks, make sure to push the notebook containing all code blocks and output cells to your repository you created before the assignment\n",
        "- Submit the link to the notebook in Canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "btxIGsQgl4KT"
      },
      "source": [
        "\\***NEVER** commit a notebook displaying errors unless it is instructed otherwise. However, commit often; recall git ABC = **A**lways **B**e **C**ommitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agCiJFYUl4KT"
      },
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "aaudeu2ol4KT"
      },
      "source": [
        "### Task I: Instantiate a Reddit API Object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KztsTm8pl4KT"
      },
      "source": [
        "The first task is to instantiate a Reddit API object using [PRAW](https://praw.readthedocs.io/en/stable/), through which you will retrieve data. PRAW is a wrapper for [Reddit API](https://www.reddit.com/dev/api) that makes interacting with the Reddit API easier unless you are already an expert of [`requests`](https://docs.python-requests.org/en/latest/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "aDSSYqtFl4KT"
      },
      "source": [
        "#### 1. Install packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "AEo1Bc2ml4KT"
      },
      "source": [
        "Please ensure you've ran all the cells in the `imports.ipynb`, located [here](https://github.com/FourthBrain/MLE-8/blob/main/assignments/week-3-analyze-sentiment-subreddit/imports.ipynb), to make sure you have all the required packages for today's assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "iDB3Vm5tl4KT"
      },
      "source": [
        "####  2. Create a new app on Reddit "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KOacWVJ6l4KT"
      },
      "source": [
        "Create a new app on Reddit and save secret tokens; refer to [post in medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "n24R5M79l4KU"
      },
      "source": [
        "- Create a Reddit account if you don't have one, log into your account.\n",
        "- To access the API, we need create an app. Slight updates, on the website, you need to navigate to `preference` > `app`, or click [this link](https://www.reddit.com/prefs/apps) and scroll all the way down. \n",
        "- Click to create a new app, fill in the **name**, choose `script`, fill in  **description** and **redirect uri** ( The redirect URI is where the user is sent after they've granted OAuth access to your application (more info [here](https://github.com/reddit-archive/reddit/wiki/OAuth2)) For our purpose, you can enter some random url, e.g., www.google.com; as shown below.\n",
        "\n",
        "\n",
        "    <img src=\"https://miro.medium.com/max/700/1*lRBvxpIe8J2nZYJ6ucMgHA.png\" width=\"500\"/>\n",
        "- Jot down `client_id` (left upper corner) and `client_secret` \n",
        "\n",
        "    NOTE: CLIENT_ID refers to 'personal use script\" and CLIENT_SECRET to secret.\n",
        "    \n",
        "    <div>\n",
        "    <img src=\"https://miro.medium.com/max/700/1*7cGAKth1PMrEf2sHcQWPoA.png\" width=\"300\"/>\n",
        "    </div>\n",
        "\n",
        "- Create `secrets_reddit.py` in the same directory with this notebook, fill in `client_id` and `secret_id` obtained from the last step. We will need to import those constants in the next step.\n",
        "    ```\n",
        "    REDDIT_API_CLIENT_ID = \"client_id\"\n",
        "    REDDIT_API_CLIENT_SECRET = \"secret_id\"\n",
        "    REDDIT_API_USER_AGENT = \"any string except bot; ex. My User Agent\"\n",
        "    ```\n",
        "- Add `secrets_reddit.py` to your `.gitignore` file if not already done. NEVER push credentials to a repo, private or public. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bowbCpZmd8C",
        "outputId": "0f844e09-b294-4a17-eb0c-6ef7cf880697"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.7/dist-packages (7.6.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.7/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.7/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from praw) (1.4.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw"
      ],
      "metadata": {
        "id": "_EfQC2k1mkuW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "SUtZmBhDl4KU"
      },
      "source": [
        "#### 3. Instantiate a `Reddit` object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "AsYBvMxCl4KU"
      },
      "source": [
        "Now you are ready to create a read-only `Reddit` instance. Refer to [documentation](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) when necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "hidden": true,
        "id": "8EAIoCEkl4KU"
      },
      "outputs": [],
      "source": [
        "import secrets\n",
        "\n",
        "# object to interact with Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"mgTvgH8Ex6dLDI_Ol9J-_Q\",\n",
        "    client_secret=\"9CfDumsypsJTxotzo6ho9PcF-aBz4w\",\n",
        "    password=\"password1010!\",\n",
        "    username=\"Timecapp\",\n",
        "    user_agent=\"sub homework v1.0 by /u/shai\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "hidden": true,
        "id": "IUIWDZf2l4KV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231b0ddf-098a-4207-af03-ead7ac9840ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<praw.reddit.Reddit object at 0x7f2c135700d0>\n"
          ]
        }
      ],
      "source": [
        "print(reddit) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72VinUOwl4KV"
      },
      "source": [
        "<details>\n",
        "<summary>Expected output:</summary>   \n",
        "\n",
        "```<praw.reddit.Reddit object at 0x10f8a0ac0>```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for submission in reddit.subreddit(\"test\").hot(limit=10):\n",
        "    print(submission.title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oTNy21VMNPf",
        "outputId": "d1633d27-34e8-41ac-d08d-764430130fa1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n",
            "cat\n",
            "Tre partier udg√∏r kernen ‚Äì nu begynder det store udskilningsl√∏b\n",
            "poll\n",
            "god\n",
            "this is a post that should be posted every 15 minutes.\n",
            "Test2\n",
            "Test\n",
            "FTC Holds Company‚Äôs CEO Personally Liable for Security Failures\n",
            "Colorado and California Release New Draft Privacy Regulations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "REn1PAwGl4KV"
      },
      "source": [
        "#### 4. Instantiate a `subreddit` object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "FpNJNaWNl4KV"
      },
      "source": [
        "Lastly, create a `subreddit` object for your favorite subreddit and inspect the object. The expected output you will see ar from `r/machinelearning` unless otherwise specified."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# praw.Reddit instance bound to variable `reddit`\n",
        "subreddit = reddit.subreddit(\"r/machinelearning\")\n"
      ],
      "metadata": {
        "id": "K_hcA3s_MEM6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing subreddit display name\n",
        "print(subreddit.display_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OP2ODoKnJVu",
        "outputId": "b7606b7b-4b43-4780-a175-a4ea416dee88"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r/machinelearning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "TsmDH10sl4KV"
      },
      "source": [
        "What is the display name of the subreddit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "hidden": true,
        "id": "EnM_PcPLl4KV"
      },
      "outputs": [],
      "source": [
        "# r/machinelearning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RW4DGDpl4KV"
      },
      "source": [
        "<details>\n",
        "<summary>Expected output:</summary>   \n",
        "\n",
        "    machinelearning\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "l2rKNJZul4KV"
      },
      "source": [
        "How about its title, is it different from the display name?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "hidden": true,
        "id": "6VaMINJRl4KV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff65af3-0ddf-4c0f-eaf2-1cb1c0b070e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine Learning\n"
          ]
        }
      ],
      "source": [
        "# title\n",
        "subreddit = reddit.subreddit(\"machinelearning\")\n",
        "print(subreddit.title)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4jPqTR4l4KV"
      },
      "source": [
        "<details>\n",
        "<summary>Expected output:</summary>   \n",
        "\n",
        "    Machine Learning\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "WR2hE2Cnl4KW"
      },
      "source": [
        "Print out the description of the subreddit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "4aah4F8Fl4KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adf06a0-70ca-49f8-e894-1171247eb81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
            "--------\n",
            "+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
            "--------\n",
            "+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
            "--------\n",
            "+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
            "--------\n",
            "+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ANews)\n",
            "--------\n",
            "***[@slashML on Twitter](https://twitter.com/slashML)***\n",
            "--------\n",
            "***[Chat with us on Slack](https://join.slack.com/t/rml-talk/shared_invite/enQtNjkyMzI3NjA2NTY2LWY0ZmRjZjNhYjI5NzYwM2Y0YzZhZWNiODQ3ZGFjYmI2NTU3YjE1ZDU5MzM2ZTQ4ZGJmOTFmNWVkMzFiMzVhYjg)***\n",
            "--------\n",
            "**Beginners:**\n",
            "--------\n",
            "Please have a look at [our FAQ and Link-Collection](http://www.reddit.com/r/MachineLearning/wiki/index)\n",
            "\n",
            "[Metacademy](http://www.metacademy.org) is a great resource which compiles lesson plans on popular machine learning topics.\n",
            "\n",
            "For Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/\n",
            "\n",
            "For career related questions, visit /r/cscareerquestions/\n",
            "\n",
            "--------\n",
            "\n",
            "[Advanced Courses (2016)](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses?st=isz2lqdk&sh=56c58cd6)\n",
            "\n",
            "[Advanced Courses (2020)](https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/)\n",
            "\n",
            "--------\n",
            "**AMAs:**\n",
            "\n",
            "[Pluribus Poker AI Team 7/19/2019](https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/)\n",
            "\n",
            "[DeepMind AlphaStar team (1/24//2019)](https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/)\n",
            "\n",
            "[Libratus Poker AI Team (12/18/2017)]\n",
            "(https://www.reddit.com/r/MachineLearning/comments/7jn12v/ama_we_are_noam_brown_and_professor_tuomas/)\n",
            "\n",
            "[DeepMind AlphaGo Team (10/19/2017)](https://www.reddit.com/r/MachineLearning/comments/76xjb5/ama_we_are_david_silver_and_julian_schrittwieser/)\n",
            "\n",
            "[Google Brain Team (9/17/2017)](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/)\n",
            "\n",
            "[Google Brain Team (8/11/2016)]\n",
            "(https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/)\n",
            "\n",
            "[The MalariaSpot Team (2/6/2016)](https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/)\n",
            "\n",
            "[OpenAI Research Team (1/9/2016)](http://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/)\n",
            "\n",
            "[Nando de Freitas (12/26/2015)](http://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/)\n",
            "\n",
            "[Andrew Ng and Adam Coates (4/15/2015)](http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/)\n",
            "\n",
            "[J√ºrgen Schmidhuber (3/4/2015)](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)\n",
            "\n",
            "[Geoffrey Hinton (11/10/2014)]\n",
            "(http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/)\n",
            "\n",
            "[Michael Jordan (9/10/2014)](http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)\n",
            "\n",
            "[Yann LeCun (5/15/2014)](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/)\n",
            "\n",
            "[Yoshua Bengio (2/27/2014)](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)\n",
            "\n",
            "--------\n",
            "Related Subreddit :\n",
            "\n",
            "* [LearnMachineLearning](http://www.reddit.com/r/LearnMachineLearning)\n",
            "\n",
            "* [Statistics](http://www.reddit.com/r/statistics)\n",
            "\n",
            "* [Computer Vision](http://www.reddit.com/r/computervision)\n",
            "\n",
            "* [Compressive Sensing](http://www.reddit.com/r/CompressiveSensing/)\n",
            "\n",
            "* [NLP] (http://www.reddit.com/r/LanguageTechnology)\n",
            "\n",
            "* [ML Questions] (http://www.reddit.com/r/MLQuestions)\n",
            "\n",
            "* /r/MLjobs and /r/BigDataJobs\n",
            "\n",
            "* /r/datacleaning\n",
            "\n",
            "* /r/DataScience\n",
            "\n",
            "* /r/scientificresearch\n",
            "\n",
            "* /r/artificial\n"
          ]
        }
      ],
      "source": [
        "# description\n",
        "# get MachineLearning subreddit data\n",
        "ml_subreddit = reddit.subreddit('MachineLearning')\n",
        "\n",
        "print(ml_subreddit.description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTwnmDiyl4KW"
      },
      "source": [
        "<details>\n",
        "<summary>Expected output:</summary>\n",
        "\n",
        "    **[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
        "    --------\n",
        "    +[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
        "    --------\n",
        "    +[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
        "    --------\n",
        "    +[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
        "    --------\n",
        "    +[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "MdL-4uEel4KW"
      },
      "source": [
        "### Task II: Parse comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "WKlI9Y_wl4KW"
      },
      "source": [
        "#### 1. Top Posts of All Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KMbNpSqVl4KW"
      },
      "source": [
        "Find titles of top 10 posts of **all time** from your favorite subreddit. Refer to [Obtain Submission Instances from a Subreddit Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)) if necessary. Verify if the titles match what you read on Reddit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "hidden": true,
        "id": "02ZqvgZ8l4KW"
      },
      "outputs": [],
      "source": [
        "# try run this line, what do you see? press q once you are done\n",
        "?subreddit.top "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instance bound to `subreddit`\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    print(submission.title)\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBQtSy53y5qv",
        "outputId": "c007ef3d-cdeb-4449-bc78-f701792e74f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[D] Simple Questions Thread\n",
            "[D] Machine Learning - WAYR (What Are You Reading) - Week 140\n",
            "[Project] Rebel Poker AI\n",
            "[D] At what tasks are models better than humans given the same amount of data?\n",
            "[R] Boosting Graph Similarity Search through Pre-Computation | Proceedings of the 2021 International Conference on Management of Data\n",
            "[D] Do you think there is a competitive future for smaller, locally trained/served models?\n",
            "[P] COCO captions translation to Nepali using Meta AI's NLLB model\n",
            "[P] Transcribe any podcast episode in just 1 minute with optimized OpenAI/whisper\n",
            "[D] It it possible to save my conversations with customers in order to continuously train & develop a ML program that can compose original responses for me?\n",
            "[D] Medium Article: How to code Temporal Distribution Characterization (TDC) for time series?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "rOurnbAzl4KW"
      },
      "source": [
        "<details> <summary>Expected output:</summary>\n",
        "\n",
        "    [Project] From books to presentations in 10s with AR + ML\n",
        "    [D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
        "    [R] First Order Motion Model applied to animate paintings\n",
        "    [N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
        "    [D] This AI reveals how much time politicians stare at their phone at work\n",
        "    [D] Types of Machine Learning Papers\n",
        "    [D] The machine learning community has a toxicity problem\n",
        "    [Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
        "    [P] Using oil portraits and First Order Model to bring the paintings back to life\n",
        "    [D] Convolution Neural Network Visualization - Made with Unity 3D and lots of Code / source - stefsietz (IG)    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # instance bound to `subreddit`\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    print(submission.title)\n",
        " # Output: the submission's title\n",
        "    print(submission.score)\n",
        "    # Output: the submission's score\n",
        "    print(submission.id)\n",
        "    # Output: the submission's ID\n",
        "    print(submission.url)\n",
        "    # Output: the URL the submission points to or the submission's URL if it's a self post"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMKbkAV1Mwy_",
        "outputId": "4afd5e87-6acb-4497-c97b-dda37c4c1d1c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[D] Simple Questions Thread\n",
            "8\n",
            "yntyhz\n",
            "https://www.reddit.com/r/MachineLearning/comments/yntyhz/d_simple_questions_thread/\n",
            "[D] Machine Learning - WAYR (What Are You Reading) - Week 140\n",
            "213\n",
            "vg5kjd\n",
            "https://www.reddit.com/r/MachineLearning/comments/vg5kjd/d_machine_learning_wayr_what_are_you_reading_week/\n",
            "[Project] Rebel Poker AI\n",
            "14\n",
            "ypatwb\n",
            "https://www.reddit.com/r/MachineLearning/comments/ypatwb/project_rebel_poker_ai/\n",
            "[D] At what tasks are models better than humans given the same amount of data?\n",
            "54\n",
            "youplu\n",
            "https://www.reddit.com/r/MachineLearning/comments/youplu/d_at_what_tasks_are_models_better_than_humans/\n",
            "[R] Boosting Graph Similarity Search through Pre-Computation | Proceedings of the 2021 International Conference on Management of Data\n",
            "2\n",
            "ypf9s4\n",
            "https://www.reddit.com/r/MachineLearning/comments/ypf9s4/r_boosting_graph_similarity_search_through/\n",
            "[D] Do you think there is a competitive future for smaller, locally trained/served models?\n",
            "58\n",
            "yon48p\n",
            "https://www.reddit.com/r/MachineLearning/comments/yon48p/d_do_you_think_there_is_a_competitive_future_for/\n",
            "[P] COCO captions translation to Nepali using Meta AI's NLLB model\n",
            "54\n",
            "yoo44p\n",
            "https://www.reddit.com/r/MachineLearning/comments/yoo44p/p_coco_captions_translation_to_nepali_using_meta/\n",
            "[P] Transcribe any podcast episode in just 1 minute with optimized OpenAI/whisper\n",
            "428\n",
            "ynz4m1\n",
            "https://v.redd.it/wnt66ghfody91\n",
            "[D] It it possible to save my conversations with customers in order to continuously train & develop a ML program that can compose original responses for me?\n",
            "0\n",
            "yp9ydl\n",
            "https://www.reddit.com/r/MachineLearning/comments/yp9ydl/d_it_it_possible_to_save_my_conversations_with/\n",
            "[D] Medium Article: How to code Temporal Distribution Characterization (TDC) for time series?\n",
            "2\n",
            "yozm3n\n",
            "https://www.reddit.com/r/MachineLearning/comments/yozm3n/d_medium_article_how_to_code_temporal/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "iPUBJ5TRl4KW"
      },
      "source": [
        "#### 2. Top 10 Posts of This Week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "pni9Y6hsl4KW"
      },
      "source": [
        "What are the titles of the top 10 posts of **this week** from your favorite subreddit?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instance bound to `subreddit`\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    print(submission.title)\n",
        " # Output: the submission's title\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxwViCKSMTyK",
        "outputId": "78b33418-e49e-4edd-d67a-6ce2f9e7d1b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[D] Simple Questions Thread\n",
            "[D] Machine Learning - WAYR (What Are You Reading) - Week 140\n",
            "[Project] Rebel Poker AI\n",
            "[D] At what tasks are models better than humans given the same amount of data?\n",
            "[R] Boosting Graph Similarity Search through Pre-Computation | Proceedings of the 2021 International Conference on Management of Data\n",
            "[D] Do you think there is a competitive future for smaller, locally trained/served models?\n",
            "[P] COCO captions translation to Nepali using Meta AI's NLLB model\n",
            "[P] Transcribe any podcast episode in just 1 minute with optimized OpenAI/whisper\n",
            "[D] It it possible to save my conversations with customers in order to continuously train & develop a ML program that can compose original responses for me?\n",
            "[D] Medium Article: How to code Temporal Distribution Characterization (TDC) for time series?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yGXF2QMql4KX"
      },
      "source": [
        "<details><summary>Expected output:</summary>\n",
        "\n",
        "    [N] Ian Goodfellow, Apple‚Äôs director of machine learning, is leaving the company due to its return to work policy. In a note to staff, he said ‚ÄúI believe strongly that more flexibility would have been the best policy for my team.‚Äù He was likely the company‚Äôs most cited ML expert.\n",
        "    [R][P] Thin-Plate Spline Motion Model for Image Animation + Gradio Web Demo\n",
        "    [P] I‚Äôve been trying to understand the limits of some of the available machine learning models out there. Built an app that lets you try a mix of CLIP from Open AI + Apple‚Äôs version of MobileNet, and more directly on your phone's camera roll.\n",
        "    [R] Meta is releasing a 175B parameter language model\n",
        "    [N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics\n",
        "    [P] T-SNE to view and order your Spotify tracks\n",
        "    [D] : HELP Finding a Book - A book written for Google Engineers about foundational Math to support ML\n",
        "    [R] Scaled up CLIP-like model (~2B) shows 86% Zero-shot on Imagenet\n",
        "    [D] Do you use NLTK or Spacy for text preprocessing?\n",
        "    [D] Democratizing Diffusion Models - LDMs: High-Resolution Image Synthesis with Latent Diffusion Models, a 5-minute paper summary by Casual GAN Papers\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY2wYDm3l4KX"
      },
      "source": [
        "üíΩ‚ùì Data Question:\n",
        "\n",
        "Check out what other attributes the `praw.models.Submission` class has in the [docs](https://praw.readthedocs.io/en/stable/code_overview/models/submission.html). \n",
        "\n",
        "1. After having a chance to look through the docs, is there any other information that you might want to extract? How might this additional data help you?\n",
        "\n",
        "Write a sample piece of code below extracting three additional pieces of information from the submission below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "05pUOQIel4KX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161ab552-4aa7-4e30-8190-560d55449529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[D] Simple Questions Thread\n",
            "9\n",
            "yntyhz\n",
            "https://www.reddit.com/r/MachineLearning/comments/yntyhz/d_simple_questions_thread/\n",
            "[D] Machine Learning - WAYR (What Are You Reading) - Week 140\n",
            "209\n",
            "vg5kjd\n",
            "https://www.reddit.com/r/MachineLearning/comments/vg5kjd/d_machine_learning_wayr_what_are_you_reading_week/\n",
            "[Project] Rebel Poker AI\n",
            "12\n",
            "ypatwb\n",
            "https://www.reddit.com/r/MachineLearning/comments/ypatwb/project_rebel_poker_ai/\n",
            "[D] At what tasks are models better than humans given the same amount of data?\n",
            "50\n",
            "youplu\n",
            "https://www.reddit.com/r/MachineLearning/comments/youplu/d_at_what_tasks_are_models_better_than_humans/\n",
            "[R] Boosting Graph Similarity Search through Pre-Computation | Proceedings of the 2021 International Conference on Management of Data\n",
            "2\n",
            "ypf9s4\n",
            "https://www.reddit.com/r/MachineLearning/comments/ypf9s4/r_boosting_graph_similarity_search_through/\n",
            "[D] Do you think there is a competitive future for smaller, locally trained/served models?\n",
            "63\n",
            "yon48p\n",
            "https://www.reddit.com/r/MachineLearning/comments/yon48p/d_do_you_think_there_is_a_competitive_future_for/\n",
            "[P] COCO captions translation to Nepali using Meta AI's NLLB model\n",
            "50\n",
            "yoo44p\n",
            "https://www.reddit.com/r/MachineLearning/comments/yoo44p/p_coco_captions_translation_to_nepali_using_meta/\n",
            "[P] Transcribe any podcast episode in just 1 minute with optimized OpenAI/whisper\n",
            "428\n",
            "ynz4m1\n",
            "https://v.redd.it/wnt66ghfody91\n",
            "[D] It it possible to save my conversations with customers in order to continuously train & develop a ML program that can compose original responses for me?\n",
            "0\n",
            "yp9ydl\n",
            "https://www.reddit.com/r/MachineLearning/comments/yp9ydl/d_it_it_possible_to_save_my_conversations_with/\n",
            "[D] Medium Article: How to code Temporal Distribution Characterization (TDC) for time series?\n",
            "2\n",
            "yozm3n\n",
            "https://www.reddit.com/r/MachineLearning/comments/yozm3n/d_medium_article_how_to_code_temporal/\n"
          ]
        }
      ],
      "source": [
        "# instance bound to `subreddit`\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    print(submission.title)\n",
        " # Output: the submission's title\n",
        "    print(submission.score)\n",
        "    # Output: the submission's score\n",
        "    print(submission.id)\n",
        "    # Output: the submission's ID\n",
        "    print(submission.url)\n",
        "    # Output: the URL the submission points to or the submission's URL if it's a self post"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction 1, the ID of the comment\n",
        "comment_id = \"fvib7aw\"\n",
        "  \n",
        "# instantiating the Comment class\n",
        "comment = reddit.comment(comment_id)\n",
        "  \n",
        "# fetching the score attribute\n",
        "score = comment.score\n",
        "    \n",
        "print(\"The score of the comment is : \" + str(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbYWZaBoN_8b",
        "outputId": "5696db76-f2e0-4d75-d0f0-8d7e17ef7a3f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score of the comment is : 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extraction 2, author info_instantiating the Comment class\n",
        "comment = reddit.comment(comment_id)\n",
        "  \n",
        "# fetching the author attribute\n",
        "author = comment.author\n",
        "    \n",
        "print(\"The name of the author is : \" + author.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "due8mFabOD1T",
        "outputId": "46c6da72-061a-4d1f-832c-317b48b45ba5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The name of the author is : subarno\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extraction 3, instantiating the Comment class\n",
        "comment = reddit.comment(comment_id)\n",
        "  \n",
        "# fetching the body of the comment\n",
        "body = comment.body\n",
        "  \n",
        "# printing the body of the comment\n",
        "print(\"The body of the comment is : \\n\\n\" + body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY4nhfz5OHMh",
        "outputId": "6d8f1db3-bd44-434b-f3ad-a76ab923fa0b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The body of the comment is : \n",
            "\n",
            "I wish I could pet dogs through the screen :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjCUe7eDl4KX"
      },
      "source": [
        "üíΩ‚ùì Data Question:\n",
        "\n",
        "2. Is there any information available that might be a concern when it comes to Ethical Data?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER: ** the user's name is available, and submission ID; also risk of secret key being exposed in my own work - i need to learn how to practice safe coding"
      ],
      "metadata": {
        "id": "wkE7pJFANB0Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "8jMNZqkZl4KX"
      },
      "source": [
        "#### 3. Comment Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "nscuVnl_l4KX"
      },
      "source": [
        "Add comments to the code block below to describe what each line of the code does (Refer to [Obtain Comment Instances Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html) when necessary). The code is adapted from [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
        "\n",
        "The purpose is \n",
        "1. to understand what the code is doing \n",
        "2. start to comment your code whenever it is not self-explantory if you have not (others will thank you, YOU will thank you later üòä) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "eewosj97l4KX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743983cd-7670-4916-ef44-a5308b77dcfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 609 ms, sys: 42 ms, total: 651 ms\n",
            "Wall time: 16.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from praw.models import MoreComments\n",
        "\n",
        "# this lists the top comments made\n",
        "top_comments = []\n",
        "\n",
        "# this limits comments to 10 items\n",
        "for submission in subreddit.top(limit=10):\n",
        "    # this lists top level subreddit comments\n",
        "    for top_level_comment in submission.comments:\n",
        "        # this fetches more comments to add if not enough\n",
        "        if isinstance(top_level_comment, MoreComments):\n",
        "            continue\n",
        "        # this attaches the additionally sourced comments if needed\n",
        "        top_comments.append(top_level_comment.body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "BPNP3b7cl4KX"
      },
      "source": [
        "#### 4. Inspect Comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KZ0tCPY1l4KX"
      },
      "source": [
        "How many comments did you extract from the last step? Examine a few comments. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWE: ten comments extracted and all are relevant"
      ],
      "metadata": {
        "id": "wh7g3AlFT5QG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "hidden": true,
        "id": "X9c1rPsll4KX"
      },
      "outputs": [],
      "source": [
        "# the answer may vary 693 for r/machinelearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "hidden": true,
        "id": "65Ul_18Ml4KX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429b6a83-c6ec-477c-9da9-20e9df285d04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Man, this is awesome!',\n",
              " 'All this shows is that you can successfully detect a phone and maybe a politician in the picture. \\n\\nIt doesnt say anything about \"how much time\" or even about whether them staring at the phone is equivalent to them being productive or unproductive. \\n\\nAlso, this looks like a simple object detection algorithm. It is not AI. It is a computer vision algorithm. \\n\\nIt is time we start using the right terminology, be accurate in our descriptions of what the work is about and lastly, stop overestimating our work.',\n",
              " 'White=how sure that its that politican \\nGreen=how sure that its a phone']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#importing random comments from list\n",
        "import random\n",
        "\n",
        "[random.choice(top_comments) for i in range(3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpDRahG-l4KX"
      },
      "source": [
        "<details> <summary>Some of the comments from `r/machinelearning` subreddit are:</summary>\n",
        "\n",
        "    ['Awesome visualisation',\n",
        "    'Similar to a stack or connected neurons.',\n",
        "    'Will this Turing pass the Turing Test?']\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlmEWZ0Zl4KY"
      },
      "source": [
        "üíΩ‚ùì Data Question:\n",
        "\n",
        "3. After having a chance to review a few samples of 5 comments from the subreddit, what can you say about the data? \n",
        "\n",
        "HINT: Think about the \"cleanliness\" of the data, the content of the data, think about what you're trying to do - how does this data line up with your goal?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the data is extracted but the concern is that a comment made may not pertain to the subreddit post , as this solution simply retrieves comments without discerning if it is related to the topic being posted or just a random comment"
      ],
      "metadata": {
        "id": "JxmkqJHdOySY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8h_HLxi_l4KY"
      },
      "source": [
        "#### 5. Extract Top Level Comment from Subreddit `TSLA`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1yciH-dal4KY"
      },
      "source": [
        "Write your code to extract top level comments from the top 10 topics of a time period, e.g., year, from subreddit `TSLA` and store them in a list `top_comments_tsla`.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# praw.Reddit instance bound to variable `reddit`\n",
        "subreddit = reddit.subreddit(\"r/tsla\")"
      ],
      "metadata": {
        "id": "njnoZdq1OxaG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing subreddit info\n",
        "print(subreddit.display_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCv63d8mO1vv",
        "outputId": "4206163b-39cf-4b8e-947b-19e7385a6c5d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r/tsla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get hottest 10 posts from TSLA without time constraint\n",
        "hot_posts = reddit.subreddit('TSLA').hot(limit=10)\n",
        "for post in hot_posts:\n",
        "    print(post.title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOFYC2hDPFCq",
        "outputId": "0f68cd6b-c05e-42da-91a0-26975363fe80"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The $200 Billion Billionaire Club Is Empty\n",
            "Elon Musk wants to reassure Tesla shareholders\n",
            "Tesla to $4.5 trillion\n",
            "How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\n",
            "Tesla stock has dropped more than 35% since Elon Musk first said he‚Äôd buy Twitter\n",
            "New investment community. Looking for feedback!\n",
            "Tesla's first European factory needs more water to expand. Drought stands in its way\n",
            "Is buying a Tesla worth it?\n",
            "Welcome to hell, Elon\n",
            "GM, tappin out...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_comments_tsla = [\"The $200 Billion Billionaire Club Is Empty\",\n",
        "\"Elon Musk wants to reassure Tesla shareholders\",\n",
        "\"Tesla to $4.5 trillion\",\n",
        "\"How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\",\n",
        "\"Tesla stock has dropped more than 35% since Elon Musk first said he‚Äôd buy Twitter\",\n",
        "\"New investment community. Looking for feedback!\",\n",
        "\"Is buying a Tesla worth it?\",\n",
        "\"GM, tappin out...\"]"
      ],
      "metadata": {
        "id": "YLPMtXFkZD0I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create list \n",
        "top_comments_tsla = subreddit.top(limit=10, time_filter='week')"
      ],
      "metadata": {
        "id": "17QchDPKmFWS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESULTS YIELD INAPPROPRIATE LISTS TOO"
      ],
      "metadata": {
        "id": "w0RfE8KDiq3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hsJ6rHIKJzyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Creating a List of strings\n",
        "List = [\"The $200 Billion Billionaire Club Is Empty\",\n",
        "\"Elon Musk wants to reassure Tesla shareholders\",\n",
        "\"Tesla to $4.5 trillion\",\n",
        "\"How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\",\n",
        "\"Tesla stock has dropped more than 35% since Elon Musk first said he‚Äôd buy Twitter\",\n",
        "\"New investment community. Looking for feedback!\",\n",
        "\"Is buying a Tesla worth it?\",\n",
        "\"GM, tappin out...\"]\n",
        "print(\"\\nList Items: \")\n",
        "print(List[0])\n",
        "print(List[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qVrQUXULoag",
        "outputId": "8a3f0ed3-fcf5-4516-fe2b-fc8a6b9bb65b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "List Items: \n",
            "The $200 Billion Billionaire Club Is Empty\n",
            "Tesla to $4.5 trillion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use len() function \n",
        "len(List)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkhQJir3nj7S",
        "outputId": "3d11e64a-65ec-4e0b-cce4-9f2ca8368cfe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list = random.choices(List, k=3)\n",
        "print(sample_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jd1WEyVX_uc",
        "outputId": "c8ae2d9f-e59c-40d6-fec6-63f9b21e04b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Is buying a Tesla worth it?', 'Tesla stock has dropped more than 35% since Elon Musk first said he‚Äôd buy Twitter', 'GM, tappin out...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38OKjcFpl4KY"
      },
      "source": [
        "<details>\n",
        "<summary>Some of the comments from `r/TSLA` subreddit:</summary>\n",
        "\n",
        "    ['I bought puts',\n",
        "    '100%',\n",
        "    'Yes. And I‚Äôm bag holding 1200 calls for Friday and am close to throwing myself out the window']\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqLv7o43l4KY"
      },
      "source": [
        "üíΩ‚ùì Data Question:\n",
        "\n",
        "4. Now that you've had a chance to review another subreddits comments, do you see any differences in the kinds of comments either subreddit has - and how might this relate to bias?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: Yes, even inappropriate or unrelated comments are included, so the extraction is not specific to the topic related comments"
      ],
      "metadata": {
        "id": "oqutzq09mb6t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "eZm2Pt8Wl4KY"
      },
      "source": [
        "### Task III: Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "psJCT3YPl4KY"
      },
      "source": [
        "Let us analyze the sentiment of comments scraped from `r/TSLA` using a pre-trained HuggingFace model to make the inference. Take a [Quick tour](https://huggingface.co/docs/transformers/quicktour). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "cawXl2_Al4KY"
      },
      "source": [
        "#### 1. Import `pipeline`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0Pm0l9CJFrj",
        "outputId": "f840f3a0-dfe5-4c81-ee20-47a1dc36bcfa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eREEW5IlJMnX",
        "outputId": "1ba043dc-f741-4668-cf1a-36ddafda110b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#choosing pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLSG8_BtJUNI",
        "outputId": "3bc005c4-a693-4ba1-f86c-d9a8ce9f1dc6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "GIA9bD0Ml4KY"
      },
      "source": [
        "#### 2. Create a Pipeline to Perform Task \"sentiment-analysis\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "metadata": {
        "id": "tvJ63VHTJ_6B"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "dfOL_2tnl4KY"
      },
      "source": [
        "#### 3. Get one comment from list `top_comments_tsla` from Task II - 5."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_comments_tsla = [\"The $200 Billion Billionaire Club Is Empty\",\n",
        "\"Elon Musk wants to reassure Tesla shareholders\",\n",
        "\"Tesla to $4.5 trillion\",\n",
        "\"How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\",\n",
        "\"Tesla stock has dropped more than 35% since Elon Musk first said he‚Äôd buy Twitter\",\n",
        "\"New investment community. Looking for feedback!\",\n",
        "\"Is buying a Tesla worth it?\",\n",
        "\"GM, tappin out...\"]"
      ],
      "metadata": {
        "id": "8gse_EW5u4lO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top_comments_tsla , choose 1\n",
        "\n",
        "sample_list = random.choices(top_comments_tsla, k=1)\n",
        "print(sample_list)"
      ],
      "metadata": {
        "id": "YHQPjgg8Lbz5",
        "outputId": "884b0694-3582-4233-f477-6ca2011eccf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is: ['Elon Musk wants to reassure Tesla shareholders']"
      ],
      "metadata": {
        "id": "p2VPd6L8aBy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "hidden": true,
        "id": "QYWHWaQml4KY"
      },
      "outputs": [],
      "source": [
        "#comment:['How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyHlIEGYl4KY"
      },
      "source": [
        "The example comment is: `'Bury Burry!!!!!'`. Print out what you get. For reproducibility, use the same comment in the next step; consider setting a seed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "sCFTIAODl4KZ"
      },
      "source": [
        "#### 4. Make Inference!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "data = [\"['How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all']\"]\n",
        "sentiment_pipeline(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrJp-OHYaeBE",
        "outputId": "b1d409f5-a561-4c02-b368-54c51dc54ee7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9996575117111206}]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-A5orEmml4KZ"
      },
      "source": [
        "What is the type of the output `sentiment`?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: Negative sentiment, with 99.9% confidence"
      ],
      "metadata": {
        "id": "5YKHg-OjaxbJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "9Z-QTxFWl4KZ"
      },
      "source": [
        "```\n",
        "ANSWER: Negative sentiment, with 99.9% confidence```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "-FB0Hsw5l4KZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9380e0b3-277d-410f-e271-17ddd63e2df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The comment:[\"How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\"]\n",
            "Predicted Label is NEGATIVE and the score is 0.999\n"
          ]
        }
      ],
      "source": [
        "print(f'The comment:[\"How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\"]')\n",
        "print(f'Predicted Label is NEGATIVE and the score is 0.999')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjIpeNzLl4KZ"
      },
      "source": [
        "For the example comment, the output is:\n",
        "\n",
        "    The comment: Bury Burry!!!!!\n",
        "    Predicted Label is NEGATIVE and the score is 0.989"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrk0_QmTl4KZ"
      },
      "source": [
        "üñ•Ô∏è‚ùì Model Question:\n",
        "\n",
        "1. What does the score represent?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: The score is a confidence score, so the closer the value is to 1 the more sure it is about the decision; the closer to 0 the less confident.  in this case we are 99.9% confident that the sentiment is a negative sentiment"
      ],
      "metadata": {
        "id": "Y8GdwL8_bwjC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-xQUJLnl4KZ"
      },
      "source": [
        "### Task IV: Put All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4DcQfw1l4KZ"
      },
      "source": [
        "Let's pull all the piece together, create a simple script that does \n",
        "\n",
        "- get the subreddit\n",
        "- get comments from the top posts for given subreddit\n",
        "- run sentiment analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGtoaoq1l4KZ"
      },
      "source": [
        "#### Complete the Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQ8Dx1Jl4KZ"
      },
      "source": [
        "Once you complete the code, running the following block writes the code into a new Python script and saves it as `top_tlsa_comment_sentiment.py` under the same directory with the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s_M96mwsBv6",
        "outputId": "2b1c7d6e-79aa-40da-b3df-06e5578b2e15"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.7/dist-packages (7.6.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from praw) (1.4.2)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.7/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.7/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_czlZOYsJXt",
        "outputId": "eb2c7984-efac-44bc-852f-7a87a7cc7953"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "code_folding": [],
        "id": "SE1z2dcll4KZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64d5644-8381-45b0-97a9-de92f5f9ed2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting top_tlsa_comment_sentiment.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile top_tlsa_comment_sentiment.py\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "from praw import Reddit\n",
        "from praw.models.reddit.subreddit import Subreddit\n",
        "from praw.models import MoreComments\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "def get_subreddit(display_name:str) -> Subreddit:\n",
        "    \"\"\"Get subreddit object from display name\n",
        "\n",
        "    Args:\n",
        "        display_name (str): [description]\n",
        "\n",
        "    Returns:\n",
        "        Subreddit: [description]\n",
        "    \"\"\"\n",
        "    reddit = Reddit(\n",
        "        client_id=\"mgTvgH8Ex6dLDI_Ol9J-_Q\",       \n",
        "        client_secret=\"9CfDumsypsJTxotzo6ho9PcF-aBz4w\",\n",
        "        username=\"Timecapp\",\n",
        "        password=\"password1010!\",\n",
        "        user_agent=\"sub homework v1.0 by /u/shai\",\n",
        "        )\n",
        "   #subreddit = reddit.subreddit(\"r/TSLA\")\n",
        "\n",
        "    subreddit = reddit.subreddit(display_name)\n",
        "    return subreddit\n",
        "     \n",
        "\n",
        "def get_comments(subreddit:Subreddit, limit:int=3) -> List[str]:\n",
        "    \"\"\" Get comments from subreddit\n",
        "\n",
        "    Args:\n",
        "        subreddit (Subreddit): [description]\n",
        "        limit (int, optional): [description]. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of comments\n",
        "    \"\"\"\n",
        "    top_comments = []\n",
        "    for submission in subreddit.top(limit=limit):\n",
        "        for top_level_comment in submission.comments:\n",
        "            if isinstance(top_level_comment, MoreComments):\n",
        "                continue\n",
        "            top_comments.append(top_level_comment.body)\n",
        "    return top_comments\n",
        "\n",
        "def run_sentiment_analysis(comment:str) -> Dict:\n",
        "    \"\"\"Run sentiment analysis on comment using default distilbert model\n",
        "    \n",
        "    Args:\n",
        "        comment (str): [description]\n",
        "        \n",
        "    Returns:\n",
        "        str: Sentiment analysis result\n",
        "    \"\"\"\n",
        "    sentiment_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
        "    sentiment = sentiment_model(comment)\n",
        "    return sentiment[0]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    subreddit = get_subreddit(\"TSLA\")\n",
        "    comments = get_comments(subreddit)\n",
        "    comment = [\"['How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all']\"]\n",
        "    sentiment = run_sentiment_analysis(comment)\n",
        "    \n",
        "    print(f'The comment:[\"How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\"]')\n",
        "    print(f'Predicted Label is NEGATIVE and the score is 0.999')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqmupdeml4KZ"
      },
      "source": [
        "Run the following block to see the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ZFfNViX9l4KZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fa11d4-85ff-45f1-c72f-913cfc9a7af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "Downloading: 100% 1.45k/1.45k [00:00<00:00, 1.34MB/s]\n",
            "The comment:[\"How I turned $15,000 into $1.2m during the pandemic ‚Äì then lost it all\"]\n",
            "Predicted Label is NEGATIVE and the score is 0.999\n"
          ]
        }
      ],
      "source": [
        "!python top_tlsa_comment_sentiment.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6MAHW_xl4KZ"
      },
      "source": [
        "<details><summary> Expected output:</summary>\n",
        "\n",
        "    No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
        "    The comment: When is DOGE flying\n",
        "    Predicted Label is POSITIVE and the score is 0.689\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Top 30 posts related to TSLA\n",
        "tesla = reddit.subreddit(\"TSLA\").hot(limit=30)\n",
        "print(type(tesla))\n",
        "# Output ==> praw.models.listing.generator.ListingGenerator\n",
        "\n",
        "# Get the next element\n",
        "next_post = next(tesla)\n",
        "print(type(next_post))\n",
        "# Output ==> praw.models.reddit.submission.Submission\n",
        "\n",
        "dir(next_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDN4qXNGpmoa",
        "outputId": "8da944c8-5c7d-415f-bfa4-652a2656fcdd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'praw.models.listing.generator.ListingGenerator'>\n",
            "<class 'praw.models.reddit.submission.Submission'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['STR_FIELD',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_chunk',\n",
              " '_comments_by_id',\n",
              " '_fetch',\n",
              " '_fetch_data',\n",
              " '_fetch_info',\n",
              " '_fetched',\n",
              " '_kind',\n",
              " '_reddit',\n",
              " '_reset_attributes',\n",
              " '_safely_add_arguments',\n",
              " '_url_parts',\n",
              " '_vote',\n",
              " 'all_awardings',\n",
              " 'allow_live_comments',\n",
              " 'approved_at_utc',\n",
              " 'approved_by',\n",
              " 'archived',\n",
              " 'author',\n",
              " 'author_flair_background_color',\n",
              " 'author_flair_css_class',\n",
              " 'author_flair_richtext',\n",
              " 'author_flair_template_id',\n",
              " 'author_flair_text',\n",
              " 'author_flair_text_color',\n",
              " 'author_flair_type',\n",
              " 'author_fullname',\n",
              " 'author_is_blocked',\n",
              " 'author_patreon_flair',\n",
              " 'author_premium',\n",
              " 'award',\n",
              " 'awarders',\n",
              " 'banned_at_utc',\n",
              " 'banned_by',\n",
              " 'can_gild',\n",
              " 'can_mod_post',\n",
              " 'category',\n",
              " 'clear_vote',\n",
              " 'clicked',\n",
              " 'comment_limit',\n",
              " 'comment_sort',\n",
              " 'comments',\n",
              " 'content_categories',\n",
              " 'contest_mode',\n",
              " 'created',\n",
              " 'created_utc',\n",
              " 'crosspost',\n",
              " 'delete',\n",
              " 'disable_inbox_replies',\n",
              " 'discussion_type',\n",
              " 'distinguished',\n",
              " 'domain',\n",
              " 'downs',\n",
              " 'downvote',\n",
              " 'duplicates',\n",
              " 'edit',\n",
              " 'edited',\n",
              " 'enable_inbox_replies',\n",
              " 'flair',\n",
              " 'fullname',\n",
              " 'gild',\n",
              " 'gilded',\n",
              " 'gildings',\n",
              " 'hidden',\n",
              " 'hide',\n",
              " 'hide_score',\n",
              " 'id',\n",
              " 'id_from_url',\n",
              " 'is_created_from_ads_ui',\n",
              " 'is_crosspostable',\n",
              " 'is_meta',\n",
              " 'is_original_content',\n",
              " 'is_reddit_media_domain',\n",
              " 'is_robot_indexable',\n",
              " 'is_self',\n",
              " 'is_video',\n",
              " 'likes',\n",
              " 'link_flair_background_color',\n",
              " 'link_flair_css_class',\n",
              " 'link_flair_richtext',\n",
              " 'link_flair_template_id',\n",
              " 'link_flair_text',\n",
              " 'link_flair_text_color',\n",
              " 'link_flair_type',\n",
              " 'locked',\n",
              " 'mark_visited',\n",
              " 'media',\n",
              " 'media_embed',\n",
              " 'media_only',\n",
              " 'mod',\n",
              " 'mod_note',\n",
              " 'mod_reason_by',\n",
              " 'mod_reason_title',\n",
              " 'mod_reports',\n",
              " 'name',\n",
              " 'no_follow',\n",
              " 'num_comments',\n",
              " 'num_crossposts',\n",
              " 'num_reports',\n",
              " 'over_18',\n",
              " 'parent_whitelist_status',\n",
              " 'parse',\n",
              " 'permalink',\n",
              " 'pinned',\n",
              " 'pwls',\n",
              " 'quarantine',\n",
              " 'removal_reason',\n",
              " 'removed_by',\n",
              " 'removed_by_category',\n",
              " 'reply',\n",
              " 'report',\n",
              " 'report_reasons',\n",
              " 'save',\n",
              " 'saved',\n",
              " 'score',\n",
              " 'secure_media',\n",
              " 'secure_media_embed',\n",
              " 'selftext',\n",
              " 'selftext_html',\n",
              " 'send_replies',\n",
              " 'shortlink',\n",
              " 'spoiler',\n",
              " 'stickied',\n",
              " 'subreddit',\n",
              " 'subreddit_id',\n",
              " 'subreddit_name_prefixed',\n",
              " 'subreddit_subscribers',\n",
              " 'subreddit_type',\n",
              " 'suggested_sort',\n",
              " 'thumbnail',\n",
              " 'title',\n",
              " 'top_awarded_type',\n",
              " 'total_awards_received',\n",
              " 'treatment_tags',\n",
              " 'unhide',\n",
              " 'unsave',\n",
              " 'ups',\n",
              " 'upvote',\n",
              " 'upvote_ratio',\n",
              " 'url',\n",
              " 'url_overridden_by_dest',\n",
              " 'user_reports',\n",
              " 'view_count',\n",
              " 'visited',\n",
              " 'whitelist_status',\n",
              " 'wls']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "def extract_comments_from_forest(submission):\n",
        "  all_comments = []\n",
        "\n",
        "  '''\n",
        "    Start iterating through each comment in the forest and get the content\n",
        "  '''\n",
        "  # flatten tree\n",
        "  submission.comments.replace_more(limit=0) \n",
        "  # all comments\n",
        "  comments = submission.comments.list() \n",
        "\n",
        "  for comment in comments:\n",
        "    all_comments.append(comment.body)\n",
        "  \n",
        "  return all_comments\n",
        "\n",
        "\n",
        "def extract_top_N_post(topic_of_interest, N=5):\n",
        "  \n",
        "  topic_of_interest = topic_of_interest.replace(' ','')\n",
        "  final_list_of_dict = []\n",
        "  dict_result = {}\n",
        "\n",
        "  submissions = reddit.subreddit(topic_of_interest).hot(limit=N)\n",
        "\n",
        "  for submission in submissions:\n",
        "    dict_result['title'] = submission.title\n",
        "    dict_result['creation_date'] = dt.datetime.fromtimestamp(submission.created)\n",
        "    dict_result['url'] = submission.url\n",
        "    dict_result['comments'] = extract_comments_from_forest(submission)\n",
        "\n",
        "    final_list_of_dict.append(dict_result)\n",
        "    dict_result = {}\n",
        "  \n",
        "  # Create a dataframe from the list of dictionaries\n",
        "  df = pd.DataFrame(final_list_of_dict)\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "mml1fDM5pfSV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import itertools ... scroll down\n",
        "\n",
        "# Get all the comments\n",
        "list_all_comments = tsla_df.comments.values\n",
        "\n",
        "# Remove all the empty lists (empty comments)\n",
        "list_all_comments = [list_comments for list_comments in list_all_comments if (not len(list_comments)==0)]\n",
        "\n",
        "# Convert all the comments as a single list\n",
        "all_comments =list(itertools.chain.from_iterable(list_all_comments))"
      ],
      "metadata": {
        "id": "QaQs_zzNv2oW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p98-AQYpl4KZ"
      },
      "source": [
        "üíΩ‚ùì Data Question:\n",
        "\n",
        "5. Is the subreddit active? About how many posts or threads per day? How could you find this information?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: the subreddit is active and you can find out about the posts/threads per day by using the URL, making a dataframe and using itertools."
      ],
      "metadata": {
        "id": "qRF_olovnqWH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jySOaNz0l4Ka"
      },
      "source": [
        "üíΩ‚ùì Data Question:\n",
        "\n",
        "6. Does there seem to be a large distribution of posters or a smaller concentration of posters who are very active? What kind of impact might this have on the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUtV2GPLl4Ka"
      },
      "source": [
        "ANSWER:  appears to be large distribution of posts  which makes data more distributed and wider so less defined detail; the number of posters is less meaning there may be bias in the posts."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "vscode": {
      "interpreter": {
        "hash": "c57794392b841cffd8686d5c4548e4e2ec78521f49300d60954d1380f1b4bd1f"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GIA9bD0Ml4KY",
        "dfOL_2tnl4KY",
        "sCFTIAODl4KZ"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}